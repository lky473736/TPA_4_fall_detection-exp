{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install thop ptflops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc9ANTSuKg5o",
        "outputId": "4d547348-b6d9-4a66-af61-32ac21113a8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.5-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from thop) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->thop) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->thop) (3.0.3)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading ptflops-0.7.5-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: thop, ptflops\n",
            "Successfully installed ptflops-0.7.5 thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "Bg_y2iUMBMgI",
        "outputId": "637e20b9-bfff-410a-a732-e20e208b0154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Loading and Balancing Training Data\n",
            "================================================================================\n",
            "Train: (6000, 200, 9), Labels: [2600 1700 1700    0]\n",
            "\n",
            "================================================================================\n",
            "Loading and Balancing Validation Data\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-951702941.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-951702941.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    392\u001b[0m                            post_sec=CONFIG['post_sec'])\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     train_X, train_y, val_X, val_y, test_X, test_y = loader.prepare_dataset(\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0mtrain_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     )\n",
            "\u001b[0;32m/tmp/ipython-input-951702941.py\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(self, train_subjects, val_subjects, test_subjects)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading and Balancing Validation Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbalance_per_subject_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_subjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_per_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Val:   {val_X.shape}, Labels: {np.bincount(val_y, minlength=4)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-951702941.py\u001b[0m in \u001b[0;36mbalance_per_subject_data\u001b[0;34m(self, subjects, samples_per_class)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mactivities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madl_activities\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfall_activities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_subject_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-951702941.py\u001b[0m in \u001b[0;36mload_subject_data\u001b[0;34m(self, subject, activities)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactivity_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sensor_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mraw_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                         \u001b[0msensor_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-951702941.py\u001b[0m in \u001b[0;36mread_sensor_file\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_sensor_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from thop import profile\n",
        "\n",
        "\n",
        "class FallDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "class SisFallLoader:\n",
        "    def __init__(self, data_path, window_size=256, stride=128, use_rate=1.0,\n",
        "                 sr=200, pre_sec=2.0, post_sec=2.0):\n",
        "        self.data_path = data_path\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.use_rate = use_rate\n",
        "        self.sr = sr\n",
        "        self.pre_len = int(sr * pre_sec)\n",
        "        self.post_len = int(sr * post_sec)\n",
        "\n",
        "        self.adl_activities = [f'D{str(i).zfill(2)}' for i in range(1, 20)]\n",
        "        self.fall_activities = [f'F{str(i).zfill(2)}' for i in range(1, 16)]\n",
        "        self.label2id = {act: 0 for act in self.adl_activities}\n",
        "        self.label2id.update({act: 1 for act in self.fall_activities})\n",
        "\n",
        "    def read_sensor_file(self, filepath):\n",
        "        with open(filepath, 'r') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        content = content.replace(' ', '')\n",
        "        rows = []\n",
        "\n",
        "        for line in content.split(';\\n'):\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    values = [float(x) for x in line.split(',')]\n",
        "                    rows.append(values)\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "\n",
        "        return np.array(rows)\n",
        "\n",
        "    def generate_pre_post_labels(self, raw_labels):\n",
        "        labels = raw_labels.copy()\n",
        "        n = len(labels)\n",
        "\n",
        "        for i in range(n):\n",
        "            if i < n - 1 and labels[i] == 1 and labels[i + 1] != 1:\n",
        "                end_idx = min(i + 1 + self.post_len, n)\n",
        "                labels[i+1:end_idx] = 3\n",
        "\n",
        "        for i in range(n):\n",
        "            if raw_labels[i] == 1 and (i == 0 or raw_labels[i-1] == 0):\n",
        "                start_idx = max(0, i - self.pre_len)\n",
        "                labels[start_idx:i] = 2\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def sliding_window(self, data, labels):\n",
        "        windows = []\n",
        "        window_labels = []\n",
        "\n",
        "        for i in range(0, len(data) - self.window_size + 1, self.stride):\n",
        "            window = data[i:i + self.window_size]\n",
        "            label_segment = labels[i:i + self.window_size]\n",
        "\n",
        "            unique, counts = np.unique(label_segment, return_counts=True)\n",
        "            label_dict = dict(zip(unique, counts))\n",
        "\n",
        "            threshold = int(self.window_size * 0.1)\n",
        "\n",
        "            if label_dict.get(1, 0) >= threshold:\n",
        "                final_label = 1\n",
        "            elif label_dict.get(3, 0) >= threshold:\n",
        "                final_label = 3\n",
        "            elif label_dict.get(2, 0) >= threshold:\n",
        "                final_label = 2\n",
        "            else:\n",
        "                final_label = 0\n",
        "\n",
        "            windows.append(window)\n",
        "            window_labels.append(final_label)\n",
        "\n",
        "        return np.array(windows), np.array(window_labels)\n",
        "\n",
        "    def load_subject_data(self, subject, activities):\n",
        "        subject_dir = os.path.join(self.data_path, subject)\n",
        "        all_data = []\n",
        "        all_labels = []\n",
        "\n",
        "        for activity in activities:\n",
        "            activity_pattern = os.path.join(subject_dir, f\"{activity}*.txt\")\n",
        "            activity_files = glob.glob(activity_pattern)\n",
        "\n",
        "            for file_path in activity_files:\n",
        "                try:\n",
        "                    raw_data = self.read_sensor_file(file_path)\n",
        "                    if raw_data is not None and raw_data.shape[1] == 9:\n",
        "                        sensor_data = raw_data\n",
        "                        activity_labels = np.full(len(sensor_data), self.label2id[activity])\n",
        "\n",
        "                        all_data.append(sensor_data)\n",
        "                        all_labels.append(activity_labels)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "        if len(all_data) == 0:\n",
        "            return None, None\n",
        "\n",
        "        return np.concatenate(all_data, axis=0), np.concatenate(all_labels)\n",
        "\n",
        "    def balance_per_subject_data(self, subjects, samples_per_class):\n",
        "        balanced_X_list = []\n",
        "        balanced_y_list = []\n",
        "\n",
        "        for subject in subjects:\n",
        "            is_elderly = subject.startswith('SE')\n",
        "            if is_elderly and subject != 'SE06':\n",
        "                activities = self.adl_activities\n",
        "            else:\n",
        "                activities = self.adl_activities + self.fall_activities\n",
        "\n",
        "            data, labels = self.load_subject_data(subject, activities)\n",
        "            if data is None:\n",
        "                continue\n",
        "\n",
        "            labels = self.generate_pre_post_labels(labels)\n",
        "\n",
        "            mean = data.mean(axis=0)\n",
        "            std = data.std(axis=0) + 1e-8\n",
        "            data = (data - mean) / std\n",
        "\n",
        "            subject_X, subject_y = self.sliding_window(data, labels)\n",
        "\n",
        "            for class_id in range(4):\n",
        "                class_mask = subject_y == class_id\n",
        "                class_indices = np.where(class_mask)[0]\n",
        "\n",
        "                if len(class_indices) > 0:\n",
        "                    if len(class_indices) >= samples_per_class:\n",
        "                        selected = np.random.choice(class_indices, samples_per_class, replace=False)\n",
        "                    else:\n",
        "                        selected = np.random.choice(class_indices, samples_per_class, replace=True)\n",
        "\n",
        "                    balanced_X_list.append(subject_X[selected])\n",
        "                    balanced_y_list.append(subject_y[selected])\n",
        "\n",
        "        return np.concatenate(balanced_X_list, axis=0), np.concatenate(balanced_y_list)\n",
        "\n",
        "    def prepare_dataset(self, train_subjects, val_subjects, test_subjects):\n",
        "        np.random.seed(42)\n",
        "        if self.use_rate < 1.0:\n",
        "            n_train = int(len(train_subjects) * self.use_rate)\n",
        "            train_subjects = np.random.choice(train_subjects, size=n_train, replace=False)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Loading and Balancing Training Data\")\n",
        "        print(\"=\"*80)\n",
        "        train_X, train_y = self.balance_per_subject_data(train_subjects, samples_per_class=100)\n",
        "        print(f\"Train: {train_X.shape}, Labels: {np.bincount(train_y, minlength=4)}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Loading and Balancing Validation Data\")\n",
        "        print(\"=\"*80)\n",
        "        val_X, val_y = self.balance_per_subject_data(val_subjects, samples_per_class=50)\n",
        "        print(f\"Val:   {val_X.shape}, Labels: {np.bincount(val_y, minlength=4)}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Loading and Balancing Test Data\")\n",
        "        print(\"=\"*80)\n",
        "        test_X, test_y = self.balance_per_subject_data(test_subjects, samples_per_class=50)\n",
        "        print(f\"Test:  {test_X.shape}, Labels: {np.bincount(test_y, minlength=4)}\")\n",
        "\n",
        "        return train_X, train_y, val_X, val_y, test_X, test_y\n",
        "\n",
        "\n",
        "class TemporalPatternAttention(nn.Module):\n",
        "    def __init__(self, filter_size, num_heads, tau, bias_scale):\n",
        "        super().__init__()\n",
        "        self.filter_size = filter_size\n",
        "        self.num_heads = num_heads\n",
        "        self.tau = tau\n",
        "        self.bias_scale = bias_scale\n",
        "\n",
        "        self.W_q = nn.Linear(filter_size, filter_size * num_heads)\n",
        "        self.W_k = nn.Linear(filter_size, filter_size * num_heads)\n",
        "        self.W_v = nn.Linear(filter_size, filter_size * num_heads)\n",
        "        self.out_proj = nn.Linear(filter_size * num_heads, filter_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        H = self.num_heads\n",
        "        d = self.filter_size\n",
        "\n",
        "        Q = self.W_q(x).view(B, T, H, d).transpose(1, 2)\n",
        "        K = self.W_k(x).view(B, T, H, d).transpose(1, 2)\n",
        "        V = self.W_v(x).view(B, T, H, d).transpose(1, 2)\n",
        "\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0).unsqueeze(0).float()\n",
        "        pos_diff = pos.transpose(-1, -2) - pos\n",
        "        bias = self.bias_scale * torch.exp(-torch.abs(pos_diff) / self.tau)\n",
        "\n",
        "        attn = torch.matmul(Q, K.transpose(-2, -1)) / (d ** 0.5) + bias.unsqueeze(1)\n",
        "        attn = torch.softmax(attn, dim=-1)\n",
        "\n",
        "        out = torch.matmul(attn, V)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, H * d)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConvEncoderTPA_FallModel(nn.Module):\n",
        "    def __init__(self, in_ch, num_classes, heads, tau, bias_scale):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_ch, 64, kernel_size=8, stride=1, padding=3)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.sc1 = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, 64, kernel_size=1, stride=1),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.sc2 = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, kernel_size=1, stride=1),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.sc3 = nn.Sequential(\n",
        "            nn.Conv1d(128, 256, kernel_size=1, stride=1),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.sc4 = nn.Sequential(\n",
        "            nn.Conv1d(256, 512, kernel_size=1, stride=1),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.tpa = TemporalPatternAttention(512, heads, tau, bias_scale)\n",
        "\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        x1_in = x\n",
        "        x1 = self.relu(self.bn1(self.conv1(x1_in)))\n",
        "        x1 = self.pool1(x1)\n",
        "        x1 = x1 + self.sc1(x1_in)\n",
        "\n",
        "        x2_in = x1\n",
        "        x2 = self.relu(self.bn2(self.conv2(x2_in)))\n",
        "        x2 = self.pool2(x2)\n",
        "        x2 = x2 + self.sc2(x2_in)\n",
        "\n",
        "        x3_in = x2\n",
        "        x3 = self.relu(self.bn3(self.conv3(x3_in)))\n",
        "        x3 = self.pool3(x3)\n",
        "        x3 = x3 + self.sc3(x3_in)\n",
        "\n",
        "        x4_in = x3\n",
        "        x4 = self.relu(self.bn4(self.conv4(x4_in)))\n",
        "        x4 = self.pool4(x4)\n",
        "        x4 = x4 + self.sc4(x4_in)\n",
        "\n",
        "        x_tpa_in = x4.transpose(1, 2)\n",
        "\n",
        "        x_att = self.tpa(x_tpa_in)\n",
        "\n",
        "        x_global = x_att.mean(dim=1)\n",
        "        x_global = self.dropout(x_global)\n",
        "\n",
        "        out = self.fc(x_global)\n",
        "        return out\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * X.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += predicted.eq(y).sum().item()\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            total_loss += loss.item() * X.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += y.size(0)\n",
        "            correct += predicted.eq(y).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    return total_loss / total, 100. * correct / total, all_preds, all_labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        'window_size': 200,\n",
        "        'stride': 100,\n",
        "        'batch_size': 512,\n",
        "        'epochs': 50,\n",
        "        'hidden_dim': 128,\n",
        "        'sisfall_path': '/content/drive/MyDrive/HAR_Dataset/SISFALL/',\n",
        "        'tpa_heads': 4,\n",
        "        'tpa_tau': 1.0,\n",
        "        'tpa_bias_scale': 1.0,\n",
        "        'class_weights': [1.0, 5.0, 8.0, 8.0],\n",
        "        'use_rate': 1.0,\n",
        "        'sr': 200,\n",
        "        'pre_sec': 2.0,\n",
        "        'post_sec': 2.0,\n",
        "    }\n",
        "\n",
        "    adult_subjects = [f'SA{str(i).zfill(2)}' for i in range(1, 24)]\n",
        "    elderly_subjects = [f'SE{str(i).zfill(2)}' for i in range(1, 16)]\n",
        "\n",
        "    train_sa = adult_subjects[:16]\n",
        "    val_sa = adult_subjects[16:19]\n",
        "    test_sa = adult_subjects[19:]\n",
        "\n",
        "    train_se = elderly_subjects[:10]\n",
        "    val_se = elderly_subjects[10:12]\n",
        "    test_se = elderly_subjects[12:]\n",
        "\n",
        "    train_split = train_sa + train_se\n",
        "    val_split = val_sa + val_se\n",
        "    test_split = test_sa + test_se\n",
        "\n",
        "    loader = SisFallLoader(CONFIG['sisfall_path'],\n",
        "                           window_size=CONFIG['window_size'],\n",
        "                           stride=CONFIG['stride'],\n",
        "                           use_rate=CONFIG['use_rate'],\n",
        "                           sr=CONFIG['sr'],\n",
        "                           pre_sec=CONFIG['pre_sec'],\n",
        "                           post_sec=CONFIG['post_sec'])\n",
        "\n",
        "    train_X, train_y, val_X, val_y, test_X, test_y = loader.prepare_dataset(\n",
        "        train_split, val_split, test_split\n",
        "    )\n",
        "\n",
        "    train_ds = FallDataset(train_X, train_y)\n",
        "    val_ds = FallDataset(val_X, val_y)\n",
        "    test_ds = FallDataset(test_X, test_y)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=CONFIG['batch_size'], shuffle=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Build Model (ConvEncoder + TPA)\")\n",
        "    print(\"=\"*80)\n",
        "    model = ConvEncoderTPA_FallModel(\n",
        "        in_ch=9,\n",
        "        num_classes=4,\n",
        "        heads=CONFIG['tpa_heads'],\n",
        "        tau=CONFIG['tpa_tau'],\n",
        "        bias_scale=CONFIG['tpa_bias_scale']\n",
        "    )\n",
        "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\" ✓ Parameters: {n_params:,}\")\n",
        "    print(f\" ✓ TPA: heads={CONFIG['tpa_heads']}, tau={CONFIG['tpa_tau']}, bias_scale={CONFIG['tpa_bias_scale']}\")\n",
        "\n",
        "\n",
        "    weights = torch.FloatTensor(CONFIG['class_weights'])\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    weights = weights.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Class Weights:\")\n",
        "    print(\"=\"*80)\n",
        "    for name, w in zip(['ADL','Fall','Pre','Post'], CONFIG['class_weights']):\n",
        "        print(f\" {name:7s}: {w:.2f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Training Start\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(CONFIG['epochs']):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d}/{CONFIG['epochs']} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_epoch = epoch + 1\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}% at Epoch {best_epoch}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    test_loss, test_acc, preds, gts = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Test Set Performance\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    print(\"\\n\" + classification_report(gts, preds, labels=[0,1,2,3],\n",
        "                                target_names=['ADL','Fall','Pre-fall','Post-fall'],\n",
        "                                digits=4, zero_division=0))\n",
        "\n",
        "    cm = confusion_matrix(gts, preds, labels=[0,1,2,3])\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['ADL','Fall','Pre','Post'],\n",
        "                yticklabels=['ADL','Fall','Pre','Post'])\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\nConfusion matrix saved to 'confusion_matrix.png'\")\n",
        "\n",
        "    dummy_input = torch.randn(1, CONFIG['window_size'], 9).to(device)\n",
        "    flops, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
        "    print(f\"\\nModel Statistics:\")\n",
        "    print(f\"  Params (M):  {params / 1e6:.2f}\")\n",
        "    print(f\"  FLOPs (M):   {flops / 1e6:.2f}\")\n",
        "\n",
        "    import time\n",
        "    model.eval()\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(100):\n",
        "            start = time.time()\n",
        "            _ = model(dummy_input)\n",
        "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "            times.append((time.time() - start) * 1000)\n",
        "    print(f\"  Inference (ms): {np.mean(times[10:]):.2f}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iV3iLEYjCdJp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}